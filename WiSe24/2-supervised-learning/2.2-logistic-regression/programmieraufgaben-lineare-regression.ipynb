{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Programmieraufgaben zu 2.2 (Logistische Regression)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aufgabe 1\n",
    "\n",
    "Sei ein Datensatz $D$ gegeben durch\n",
    "\n",
    "$$\n",
    "D=\\{((16,22),1),((43,70),1),((135,50),0),((42,46),0),((60,59),0),((105,140),1)\\}\n",
    "$$\n",
    "\n",
    "**Aufgabe**: Bestimmen Sie, unter Nutzung von Python und scikit-learn, $\\theta=(\\theta_0,\\theta_1,\\theta_2)$ für das optimale logistische Modell $h^\\text{logit}_\\theta$ bzgl. $D$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "theta_1: 1.762\n",
      "theta_2: -0.586\n",
      "theta_3: 0.453\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "D = [\n",
    "    ((16, 22), 1),\n",
    "    ((43, 70), 1),\n",
    "    ((135, 50), 0),\n",
    "    ((42, 46), 0),\n",
    "    ((60, 59), 0),\n",
    "    ((105, 140), 1),\n",
    "]\n",
    "x_vals = [d[0] for d in D]\n",
    "y_hats = [d[1] for d in D]\n",
    "\n",
    "regressor = LogisticRegression().fit(x_vals, y_hats)\n",
    "\n",
    "thetas = *regressor.intercept_, *regressor.coef_[0]\n",
    "for i, theta in enumerate(thetas, start=1):\n",
    "    print(f\"theta_{i}: {theta:0.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aufgabe 2\n",
    "\n",
    "Sei ein Datensatz $D$ gegeben durch\n",
    "\n",
    "$$\n",
    "D = \\{((12,11),0), ((13,14),1), ((11,8),1), ((6,2),0), ((12,10),0), ((7,11),0)\\}\n",
    "$$\n",
    "\n",
    "**Aufgabe**: Bestimmen Sie, unter Nutzung von Python und scikit-learn, $\\theta=(\\theta_0,\\theta_1,\\theta_2,\\theta_3,\\theta_4,\\theta_5)$ für das optimale logistische Modell $h_\\theta^{\\text{logit}}$ bzgl. $D'$, wobei $D'$ die polynomielle Merkmalserweiterung von $D$ mit Maximalgrad $2$ ist.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "theta_1: -11.265\n",
      "theta_2: 0.064\n",
      "theta_3: -0.202\n",
      "theta_4: 0.386\n",
      "theta_5: -0.635\n",
      "theta_6: 0.330\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "D = [\n",
    "    ((12, 11), 0),\n",
    "    ((13, 14), 1),\n",
    "    ((11, 8), 1),\n",
    "    ((6, 2), 0),\n",
    "    ((12, 10), 0),\n",
    "    ((7, 11), 0),\n",
    "]\n",
    "\n",
    "x_vals = [d[0] for d in D]\n",
    "y_hats = [d[1] for d in D]\n",
    "\n",
    "x_prime = PolynomialFeatures(degree=2, include_bias=False).fit_transform(x_vals)\n",
    "\n",
    "regressor = LogisticRegression().fit(x_prime, y_hats)\n",
    "\n",
    "thetas = regressor.intercept_[0], *regressor.coef_[0]\n",
    "\n",
    "for i, theta in enumerate(thetas, start=1):\n",
    "    print(f\"theta_{i}: {theta:0.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aufgabe 3\n",
    "\n",
    "Sei ein Datensatz D gegeben durch\n",
    "\n",
    "$$\n",
    "D = \\{((1,80),2),((20,3),2),((5,0),1),((13,12),1),((60,30),3),((10,40),3)\\}\n",
    "$$\n",
    "\n",
    "Führen Sie, unter Nutzung von Python und scikit-learn, eine Mehrklassen-Klassifikation mithilfe logistischer Regression durch.\n",
    "\n",
    "Erstellen Sie dazu klassenspezifische Versionen $D_c$, wobei $(c\\in \\{1, \\ldots, k\\}$ von Datensatz $D$ und bestimmen Sie jeweils $\\theta_c=(\\theta_{c,0},\\theta_{c,1},\\theta_{c,2})$ für das optimale logistische Modell $h_\\theta^{\\mathrm{logit}}$ bzgl. $D_c$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class-1: [((1, 80), 0), ((20, 3), 0), ((5, 0), 1), ((13, 12), 1), ((60, 30), 0), ((10, 40), 0)]\n",
      "class-2: [((1, 80), 1), ((20, 3), 1), ((5, 0), 0), ((13, 12), 0), ((60, 30), 0), ((10, 40), 0)]\n",
      "class-3: [((1, 80), 0), ((20, 3), 0), ((5, 0), 0), ((13, 12), 0), ((60, 30), 1), ((10, 40), 1)]\n",
      "-------------\n",
      "cls: 1\n",
      "\n",
      "theta_1: 14.690\n",
      "theta_2: -0.789\n",
      "theta_3: -0.246\n",
      "-------------\n",
      "cls: 2\n",
      "\n",
      "theta_1: -0.917\n",
      "theta_2: -0.030\n",
      "theta_3: 0.024\n",
      "-------------\n",
      "cls: 3\n",
      "\n",
      "theta_1: -5.138\n",
      "theta_2: 0.196\n",
      "theta_3: 0.053\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "D = [\n",
    "    ((1, 80), 2),\n",
    "    ((20, 3), 2),\n",
    "    ((5, 0), 1),\n",
    "    ((13, 12), 1),\n",
    "    ((60, 30), 3),\n",
    "    ((10, 40), 3),\n",
    "]\n",
    "\n",
    "x_vals = [d[0] for d in D]\n",
    "y_hats = [d[1] for d in D]\n",
    "\n",
    "y_hats_cls_1 = [1 if y == 1 else 0 for y in y_hats]\n",
    "y_hats_cls_2 = [1 if y == 2 else 0 for y in y_hats]\n",
    "y_hats_cls_3 = [1 if y == 3 else 0 for y in y_hats]\n",
    "\n",
    "print(\"class-1:\", list((x, y) for x, y in zip(x_vals, y_hats_cls_1)))\n",
    "print(\"class-2:\", list((x, y) for x, y in zip(x_vals, y_hats_cls_2)))\n",
    "print(\"class-3:\", list((x, y) for x, y in zip(x_vals, y_hats_cls_3)))\n",
    "\n",
    "regressor = LogisticRegression(multi_class=\"ovr\").fit(x_vals, y_hats)\n",
    "\n",
    "thetas_1 = regressor.intercept_[0], *regressor.coef_[0]\n",
    "thetas_2 = regressor.intercept_[1], *regressor.coef_[1]\n",
    "thetas_3 = regressor.intercept_[2], *regressor.coef_[2]\n",
    "\n",
    "for i_cls, thetas in enumerate([thetas_1, thetas_2, thetas_3], start=1):\n",
    "    print(\"-------------\")\n",
    "    print(f\"cls: {i_cls}\")\n",
    "    print()\n",
    "    for i, theta in enumerate(thetas, start=1):\n",
    "        (\"thetas:  \", [round(intercept, 3), *round(coef, 3)])\n",
    "        print(f\"theta_{i}: {theta:0.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aufgabe 4\n",
    "\n",
    "Sei ein Datensatz D gegeben durch\n",
    "\n",
    "$$\n",
    "D = \\{((0.3,0.3),1),((0.24,0.26),4),((0.33,0.2),2),((0.35,0.4),3),((0.39,0.38),3),((0.42,0.26),1)\\}\n",
    "$$\n",
    "\n",
    "Führen Sie, unter Nutzung von Python und scikit-learn, eine Mehrklassen-Klassifikation mithilfe logistischer Regression durch.\n",
    "\n",
    "Erstellen Sie dazu klassenspezifische Versionen $D'_c$, wobei $c\\in \\{1, \\ldots, k\\}$ von Datensatz $D'$ und bestimmen Sie jeweils $\\theta_c=(\\theta_{c,0},\\theta_{c,1},\\theta_{c,2},\\theta_{c,3},\\theta_{c,4},\\theta_{c,5})$ für das optimale logistische Modell $h_\\theta^{\\text{logit}}$ bzgl. $D'_c$.\n",
    "\n",
    "Dabei ist $D'$ die polynomielle Merkmalserweiterung von $D$ mit Maximalgrad $2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D':    [((0.3, 0.3, 0.09, 0.09, 0.09), 1), ((0.24, 0.26, 0.058, 0.062, 0.068), 4), ((0.33, 0.2, 0.109, 0.066, 0.04), 2), ((0.35, 0.4, 0.122, 0.14, 0.16), 3), ((0.39, 0.38, 0.152, 0.148, 0.144), 3), ((0.42, 0.26, 0.176, 0.109, 0.068), 1)]\n",
      "D'_1:  [((0.3, 0.3, 0.09, 0.09, 0.09), 1), ((0.24, 0.26, 0.058, 0.062, 0.068), 0), ((0.33, 0.2, 0.109, 0.066, 0.04), 0), ((0.35, 0.4, 0.122, 0.14, 0.16), 0), ((0.39, 0.38, 0.152, 0.148, 0.144), 0), ((0.42, 0.26, 0.176, 0.109, 0.068), 1)]\n",
      "D'_2:  [((0.3, 0.3, 0.09, 0.09, 0.09), 0), ((0.24, 0.26, 0.058, 0.062, 0.068), 0), ((0.33, 0.2, 0.109, 0.066, 0.04), 1), ((0.35, 0.4, 0.122, 0.14, 0.16), 0), ((0.39, 0.38, 0.152, 0.148, 0.144), 0), ((0.42, 0.26, 0.176, 0.109, 0.068), 0)]\n",
      "D'_3:  [((0.3, 0.3, 0.09, 0.09, 0.09), 0), ((0.24, 0.26, 0.058, 0.062, 0.068), 0), ((0.33, 0.2, 0.109, 0.066, 0.04), 0), ((0.35, 0.4, 0.122, 0.14, 0.16), 1), ((0.39, 0.38, 0.152, 0.148, 0.144), 1), ((0.42, 0.26, 0.176, 0.109, 0.068), 0)]\n",
      "D'_4:  [((0.3, 0.3, 0.09, 0.09, 0.09), 0), ((0.24, 0.26, 0.058, 0.062, 0.068), 1), ((0.33, 0.2, 0.109, 0.066, 0.04), 0), ((0.35, 0.4, 0.122, 0.14, 0.16), 0), ((0.39, 0.38, 0.152, 0.148, 0.144), 0), ((0.42, 0.26, 0.176, 0.109, 0.068), 0)]\n",
      "----------\n",
      "class-1\n",
      "\n",
      "thetas:   [-0.696, 0.043, -0.04, 0.03, -0.006, -0.032]\n",
      "theta_1:  -0.696\n",
      "theta_2:  0.043\n",
      "theta_3:  -0.040\n",
      "theta_4:  0.030\n",
      "theta_5:  -0.006\n",
      "theta_6:  -0.032\n",
      "----------\n",
      "class-2\n",
      "\n",
      "thetas:   [-1.567, -0.008, -0.099, -0.009, -0.036, -0.055]\n",
      "theta_1:  -1.567\n",
      "theta_2:  -0.008\n",
      "theta_3:  -0.099\n",
      "theta_4:  -0.009\n",
      "theta_5:  -0.036\n",
      "theta_6:  -0.055\n",
      "----------\n",
      "class-3\n",
      "\n",
      "thetas:   [-0.791, 0.062, 0.178, 0.038, 0.082, 0.113]\n",
      "theta_1:  -0.791\n",
      "theta_2:  0.062\n",
      "theta_3:  0.178\n",
      "theta_4:  0.038\n",
      "theta_5:  0.082\n",
      "theta_6:  0.113\n",
      "----------\n",
      "class-4\n",
      "\n",
      "thetas:   [-1.551, -0.098, -0.04, -0.06, -0.04, -0.027]\n",
      "theta_1:  -1.551\n",
      "theta_2:  -0.098\n",
      "theta_3:  -0.040\n",
      "theta_4:  -0.060\n",
      "theta_5:  -0.040\n",
      "theta_6:  -0.027\n"
     ]
    }
   ],
   "source": [
    "from numpy import round\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "D = [\n",
    "    ((0.3, 0.3), 1),\n",
    "    ((0.24, 0.26), 4),\n",
    "    ((0.33, 0.2), 2),\n",
    "    ((0.35, 0.4), 3),\n",
    "    ((0.39, 0.38), 3),\n",
    "    ((0.42, 0.26), 1),\n",
    "]\n",
    "x_vals = [d[0] for d in D]\n",
    "y_hats = [d[1] for d in D]\n",
    "\n",
    "x_prime = PolynomialFeatures(degree=2, include_bias=False).fit_transform(x_vals)\n",
    "x_prime = [tuple(round(x, 3)) for x in x_prime]  # ensure pretty print by rounding\n",
    "\n",
    "y_hats_cls_1 = [1 if y == 1 else 0 for y in y_hats]\n",
    "y_hats_cls_2 = [1 if y == 2 else 0 for y in y_hats]\n",
    "y_hats_cls_3 = [1 if y == 3 else 0 for y in y_hats]\n",
    "y_hats_cls_4 = [1 if y == 4 else 0 for y in y_hats]\n",
    "\n",
    "print(\"D':   \", list(zip(x_prime, y_hats)))\n",
    "print(\"D'_1: \", [(x, y) for x, y in zip(x_prime, y_hats_cls_1)])\n",
    "print(\"D'_2: \", [(x, y) for x, y in zip(x_prime, y_hats_cls_2)])\n",
    "print(\"D'_3: \", [(x, y) for x, y in zip(x_prime, y_hats_cls_3)])\n",
    "print(\"D'_4: \", [(x, y) for x, y in zip(x_prime, y_hats_cls_4)])\n",
    "\n",
    "regressor = LogisticRegression(multi_class=\"ovr\").fit(x_prime, y_hats)\n",
    "\n",
    "for cls, thetas in enumerate(zip(regressor.intercept_, regressor.coef_), start=1):\n",
    "    print(\"----------\")\n",
    "    print(f\"class-{cls}\")\n",
    "    print()\n",
    "    intercept, coef = thetas\n",
    "    print(\"thetas:  \", [round(intercept, 3), *round(coef, 3)])\n",
    "    for i, theta in enumerate((intercept, *coef), start=1):\n",
    "        print(f\"theta_{i}:  {theta:0.3f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
