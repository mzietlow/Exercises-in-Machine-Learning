{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rechenaufgaben zu 4.3 (Aktives Reinforcement Learning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rechenaufgabe\n",
    "\n",
    "Betrachten wir erneut die vollautomatisierte Packstation aus Aufgabe 4.2, in der ein Transportroboter eigenständig Pakete von der Annahmestelle in die Regale transportiert. \n",
    "\n",
    "Die Menge der Zustände sei nach wie vor:\n",
    "\n",
    "$$  \n",
    "S_\\text{tr} = \\{s_0=s_a^{v},s_a^{u},s_r^{v},s_r^{u},s_r^{w},s^t=s_a^{w}\\}\n",
    "$$\n",
    "\n",
    "und die Menge der Aktionen des Transporters ist\n",
    "\n",
    "$$\n",
    "A_\\text{tr}=\\{\\textit{move},\\textit{load},\\textit{unload}\\}\n",
    "$$\n",
    "\n",
    "Weiterhin sei die Strategie $\\pi_\\text{tr}$ gegeben:\n",
    "\n",
    "$$\n",
    "\\pi_\\text{tr}(s_a^{v})  = \\textit{load}\\\\    \\pi_\\text{tr}(s_r^{v})  = \\pi_\\text{tr}(s_r^{w}) = \\pi_\\text{tr}(s_a^{u}) = \\textit{move}\\\\    \\pi_\\text{tr}(s_r^{u})  = \\textit{unload}\n",
    "$$\n",
    "\n",
    "**Aufgabe:**\n",
    "\n",
    "Aktualisieren Sie die Q-Funktion für die Beobachtungen $o_1=(s_r^{u},\\textit{unload},s_r^{w},8), o_2=(s_a^{u},\\textit{move},s_r^{u},-1)$ und $o_3=(s_r^{u},\\textit{move},s_a^{u},0)$, wobei der Lernparameter $\\alpha=0.2$, der Discountfaktor $\\gamma=0.9$ und die Initialwerte der Q-Funktion mit $q(a,s)=0$ für alle Aktionen $a$ und Zustände $s$ angenommen werden sollen.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "q^gamma(unload, s^u_r) = 1.600\n",
      "q^gamma(move, s^u_a) = 0.088\n",
      "q^gamma(move, s^u_r) = 0.016\n"
     ]
    }
   ],
   "source": [
    "state_space = [\"s_a^v\", \"s_a^u\", \"s_r^v\", \"s_r^u\", \"s_r^w\", \"s_a^w\"]\n",
    "action_space = [\"move\", \"load\", \"unload\"]\n",
    "\n",
    "strategy = {\n",
    "    \"s_a^v\": \"load\",\n",
    "    \"s_r^v\": \"move\",\n",
    "    \"s_r^w\": \"move\",\n",
    "    \"s_a^u\": \"move\",\n",
    "    \"s_r^u\": \"unload\",\n",
    "}\n",
    "\n",
    "o_1 = (\"s_r^u\", \"unload\", \"s_r^w\", 8)\n",
    "o_2 = (\"s_a^u\", \"move\", \"s_r^u\", -1)\n",
    "o_3 = (\"s_r^u\", \"move\", \"s_a^u\", 0)\n",
    "\n",
    "observations = [o_1, o_2, o_3]\n",
    "\n",
    "q = {(action, state): 0 for state in state_space for action in action_space}\n",
    "\n",
    "alpha = 0.2\n",
    "gamma = 0.9\n",
    "\n",
    "for observation in observations:\n",
    "    first_state, current_action, next_state, reward = observation\n",
    "\n",
    "    current_q_value = q[(current_action, first_state)]\n",
    "    max_possible_next_q_value = max(\n",
    "        [q[(action, next_state)] for action in action_space]\n",
    "    )\n",
    "    q_value_update = current_q_value + alpha * (\n",
    "        reward + gamma * max_possible_next_q_value - current_q_value\n",
    "    )\n",
    "\n",
    "    q[(current_action, first_state)] = q_value_update\n",
    "\n",
    "print(f\"q^gamma(unload, s^u_r) = {q[('unload', 's_r^u')]:0.3f}\")\n",
    "print(f\"q^gamma(move, s^u_a) = {q[('move', 's_a^u')]:0.3f}\")\n",
    "print(f\"q^gamma(move, s^u_r) = {q[('move', 's_r^u')]:0.3f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
